---
title: "Logistic-LASSO Breast Cancer Classification Task"
author: "Ngoc Duong - nqd2000"
date: "3/25/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)
library(corrplot)
library(corrr)

set.seed(2020)
```


Data import and cleaning 

```{r}
breast_cancer_data = read.csv("./breast-cancer-1.csv")

bcdf = breast_cancer_data %>% 
  mutate(diagnosis = ifelse(diagnosis == "M",1,0)) %>% 
  dplyr::select(diagnosis, everything()) %>% 
  dplyr::select(-id, -X)
```

Standardize design matrix (because although logistic is scale-invariant, LASSO is not, this is to ensure comparability of estimates by these different models)

```{r}
pred_names = bcdf %>% dplyr::select(-diagnosis) %>% names() %>% as.vector()
bcdf_x = NULL

for (i in pred_names) {
col = (bcdf[,i] - mean(bcdf[,i]))/sd(bcdf[,i])
bcdf_x = cbind(bcdf_x , col)
}

colnames(bcdf_x) <- c(pred_names)

bcdf_fin = cbind(bcdf[1], bcdf_x)
```

Try to git a logistic regression model using glm package. The warning messages show algorithm did not converge, potentially due to multicollinearity.

```{r}
log.mod = glm(diagnosis~., data = bcdf_fin, family = "binomial")
summary(log.mod)
```

Investigate multicollinearity problem

```{r}
bcdf_fin %>% dplyr::select(-diagnosis) %>%  #filter only numeric variables 
  cor() %>%   
  #compute correlation matrix
  corrplot(method = "circle",         #represent correlation in "circle", size = magnitude 
           type = "upper", 
           diag=FALSE
           )
#We can see that there are some very strong correlations between certain variables
```


Find correlation pairs that are above 0.6 to leave out of the dataset

```{r}
#obtain list of variables that are correlated with one another whose correlation is at least 0.85
a = bcdf_x %>% 
    correlate() %>% 
    stretch() %>% 
    arrange(desc(r)) %>% 
    filter(r > 0.85) %>% 
    slice(which(row_number() %% 2 == 0)) %>% 
    pivot_longer(x:y) %>% dplyr::select(-r,-name) %>% distinct(value) 

#bcdf_x = as.data.frame(bcdf_x)[a$value]

new_bcdf = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -concave.points_worst, -concavity_worst, -compactness_worst, -compactness_mean, -diagnosis) 

bcdf_with_y = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -concave.points_worst, -concavity_worst, -compactness_worst, -compactness_mean) 

bcdf_x = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -concave.points_worst, -concavity_worst, -compactness_worst, -compactness_mean, -diagnosis) %>% mutate(intercept = 1)
```


### Task 1
\item Build a logistic model to classify the images into  malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.  

```{r}
# Function to compute the loglikelihood, the gradient, and the Hessian matrix for data dat evaluated at the parameter value betavec
## dat    - A list with components
#  x      - vector of explanatory variables
#  y      - vector of corresponding (binary) response variables
# betavec - [beta_0, beta_1, ..., beta_n] - the vector of parameter
#             values at which to evaluate these quantities

## Returns a list with the following components evaluated at beta
#  loglik - (scalar) the log likelihood
#  grad   - (vector of length 2) gradient
#  Hess   - (2 x 2 matrix) Hessian#
```

Function to return log-likelihood, gradient, and Hessian matrix of logistic regression

```{r}
logisticstuff <- function(y, x, betavec) {
  u <- x %*% betavec
  expu <- exp(u)
  loglik.ind = NULL
  #for(i in 1:length(y)) {
    #loglik.ind[i] = y[i]*u[i] - log(1+ expu)
    #}
  #loglik = sum(loglik.ind)
  loglik = t(u) %*% y - sum((log(1+expu)))
  # Log-likelihood at betavec
  
  p <- expu / (1 + expu)
  # P(Y_i=1|x_i)
  
  #grad = NULL
  #for(i in 1:length(betavec)){
    #grad[i] = sum(t(x[,i])%*%(dat$y - p))
  #}
  grad = t(x) %*% (y-p)
   #gradient at betavec
  
    # Hessian at betavec
  hess <- -t(x) %*% diag(as.vector(p*(1-p))) %*% x
  return(list(loglik = loglik, grad = grad, Hess = hess))
}
```

Newton-Raphson with gradient ascent and step-halving

```{r}
NewtonRaphson <- function(y, x, func, start, tol=1e-10, maxiter = 200) {
  i <- 0
  cur <- start
  x = as.matrix(x)
  colnames(x) = names(bcdf_x)
  stuff <- func(y, x , cur)
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf
  while(i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i <- i + 1
    prevloglik <- stuff$loglik
    prev <- cur
    grad <- stuff$grad
    hess <- stuff$Hess
    
    #gradient descent 
    if(t(grad) %*% hess %*% grad > 0){#positive definite matrix
    inv.hess = 
      solve(hess - (max(diag(hess))+100)*diag(nrow(hess)))} #make positive definite matrix negative definite
    else 
    {inv.hess <- solve(hess)}
    
    cur <- prev - inv.hess%*%grad
    stuff <- func(y, x, cur)
    
    #step-halving
    step = 0
    while (prevloglik > stuff$loglik){#moving too far -> halve step
    step = step + 1 
    cur <- prev - (1/2)^step * inv.hess%*%grad
    stuff <- func(y, x, cur)
    }
  res <- rbind(res, c(i, stuff$loglik, cur))
  }
  return(res)
  }
```

Test on dataset 
```{r}
NewtonRaphson(y = bcdf_fin$diagnosis, as.matrix(bcdf_x), logisticstuff, start = rep(0, ncol(bcdf_x)))

#start = rep(1, ncol(bcdf_x))
#as.matrix(bcdf_x) %*%rep(1, ncol(bcdf_x))

#t(as.matrix(bcdf_x)) %*% as.matrix(bcdf_fin[,1])
```


Logistic-LASSO

Coordinate-wise descent LASSO
```{r}
cord.lasso = function(lambda, y, X, betavec, tol = 1e-7, maxiter = 200){
  i = 0
  #pp = length(s)
  #n = length(y)
  #betavec = start
  loglik = 1e6 #reasoning?
  res = c(0, loglik, betavec)
  prevloglik = Inf
  while (i < maxiter && abs(loglik - prevloglik) > tol){
    i = i + 1
    prevloglik = loglik
    for (n in 1:length(betavec)){
      u = X %*% betavec 
      expu = exp(u)
      p = expu/(expu + 1)
      w = p*(1-p) #weight 
      
      #avoid coefficients from divergence to achieve final fitted probabilities of 0 or 1
      w = ifelse(abs(w-0) < 1e-7, 1e-7, w)
      z = u + (y-p)/w
      z_without_j = X[,-n] %*% betavec[-n]
      
     r = y - X%*% betavec
      # soft-threshold solution 
     xr = sum(X[,n]*r)
     xx = mean(X[,n]^2)   
     betavec[n] = mean(w*X[,n]*(z-z_without_j))/(mean(w*xx))
     betavec[n] = sign(xr)*ifelse(betavec[n]>0,betavec[n],0)
  #residuals
     r = r - X[,n]*betavec[n]
        
     #beta_cd[nl,] = betavec
    loglik = sum(w*(z-X %*% betavec)^2)/(2*length(y)) + lambda*sum(abs(betavec))
    res = rbind(res, c(i, loglik, betavec))}}
  return(res)
}
```

Original codes
```{r}
sfun = function(beta, lambda) {sign(beta)*max(abs(beta)-lambda,0)}
cord.lasso = function(lambda, y, X, betavec, tol = 1e-7, maxiter = 200){
  i = 0
  #pp = length(s)
  #n = length(y)
  #betavec = start
  loglik = 1e6 
  res = c(0, loglik, betavec)
  prevloglik = Inf
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf){
    i = i + 1
    prevloglik = loglik
    for (n in 1:length(betavec)){
      u = X %*% betavec 
      expu = exp(u)
      p = expu/(expu + 1)
      w = p*(1-p) #weight 
      
      #avoid coefficients from divergence to achieve final fitted probabilities of 0 or 1
      w = ifelse(abs(w-0) < 1e-7, 1e-7, w)
      z = u + (y-p)/w
      z_without_j = X[,-n] %*% betavec[-n]
      betavec[n] = sfun(mean(w*X[,n]*(z-z_without_j)),lambda)/(mean(w*X[,n]*X[,n]))}
    loglik = sum(w*(z-X %*% betavec)^2)/(2*length(y)) + lambda*sum(abs(betavec))
    res = rbind(res, c(i, loglik, betavec))}
  return(res)
}
```

```{r}
cord.lasso(lambda = 0.3, y = bcdf_fin$diagnosis, X = as.matrix(bcdf_x), betavec = rep(1, ncol(bcdf_x)))
```


### Check for convergence

Pathwise coordinate optimization to get PATH OF SOLUTIONS

```{r}
path = function(X, y, tunegrid){
  beta = NULL
  for (i in 1:100){
    cor.result = cord.lasso(lambda = tunegrid[i],
                            X = as.matrix(X), 
                            y = y,
                            betavec = rep(1, ncol(X)))
    last_beta = cor.result[nrow(cor.result),3:ncol(cor.result)]
    start = last_beta
    beta = rbind(beta, c(last_beta))
  }
  return(cbind(tunegrid,beta))
}

path.out = path(X = bcdf_x, y = bcdf_fin$diagnosis, tunegrid = exp(seq(-8e-1, -8, length = 100)))
colnames(path.out) = c("tunegrid",colnames(new_bcdf), "intercept")
path.out = as.data.frame(path.out)
```


Plot the path
```{r}
path.out %>% 
  gather(key = par, value = estimate, c(2:ncol(path.out))) %>% 
  ggplot(aes(x = log(tunegrid), y = estimate, group = par, col = par)) + 
  geom_line()
```

#Cross validation

```{r}
cv.errors = NULL
cv.error = NULL
cv.se = NULL
grid = NULL
tunegrid = exp(seq(-1,7,length = 100))
start = rep(0, ncol(bcdf_x))
i = 0
crossval = function(X, y, tunegrid, fold_num){
  folds = sample(1:fold_num, nrow(X), replace = TRUE)
  start = rep(1, ncol(X))
for(l in tunegrid){
  i = i + 1
  for(k in 1:fold_num){
  x_train = X[folds != k,] 
  y_train = y[folds != k] 
  loglasso_res = cord.lasso(lambda = l, 
                            y = y_train, 
                            X = as.matrix(x_train), 
                            betavec = start)
  loglasso_coeff = loglasso_res[nrow(loglasso_res),3:ncol(loglasso_res)]
  x_test = as.matrix(X[folds == k,]) 
  y_test = y[folds == k]
  u = x_test %*% loglasso_coeff
  expu = exp(u)
  p = expu/(1+expu)
  cv.errors[i] = mean((y_test-p)^2)
  start = loglasso_coeff
  }
  cv.error[i] = mean(cv.errors)
  cv.se[i] = sqrt(var(cv.errors)/fold_num)
  grid[i] = l
  res = cbind(grid, cv.error,cv.se)
}
  return(res)}

cv_res = crossval(X = bcdf_x, y = bcdf_fin$diagnosis, tunegrid = exp(seq(-7,2,length = 100)), fold_num = 5) %>% as_tibble()
```

```{r}
ggplot(cv_res, aes(x = log(cv_res$grid), y = cv_res$cv.error)) + 
  geom_errorbar(aes(ymin = cv_res$cv.error-cv_res$cv.se,
                    ymax = cv_res$cv.error + cv_res$cv.se),col = 1) + 
  geom_line() + geom_point(size = 0.8, col = 4)
```


# Perform cross-validation logistic LASSO in glmnet (for comparison)

```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -2, length = 20)))
set.seed(1)
model.glmn <- train(x = dat[rowTrain,1:8],
                    y = dat$diabetes[rowTrain],
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

plot(model.glmn, xTrans = function(x) log(x))   

model.glmn$bestTune
```

