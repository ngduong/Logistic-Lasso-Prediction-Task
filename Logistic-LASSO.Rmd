---
title: "Logistic-LASSO Breast Cancer Classification Task"
author: "Ngoc Duong - nqd2000"
date: "3/25/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)
library(corrplot)
library(corrr)

set.seed(2020)
```


Data import and cleaning 

```{r}
breast_cancer_data = read.csv("./breast-cancer-1.csv")

bcdf = breast_cancer_data %>% 
  mutate(diagnosis = ifelse(diagnosis == "M",1,0)) %>% 
  dplyr::select(diagnosis, everything()) %>% 
  dplyr::select(-id, -X)
```

Standardize design matrix (because although logistic is scale-invariant, LASSO is not, this is to ensure comparability of estimates by these different models)

```{r}
pred_names = bcdf %>% dplyr::select(-diagnosis) %>% names() %>% as.vector()
bcdf_x = NULL

for (i in pred_names) {
col = (bcdf[,i] - mean(bcdf[,i]))/sd(bcdf[,i])
bcdf_x = cbind(bcdf_x , col)
}

colnames(bcdf_x) <- c(pred_names)

bcdf_fin = cbind(bcdf[1], bcdf_x)
```

Try to git a logistic regression model using glm package. The warning messages show algorithm did not converge, potentially due to multicollinearity.

```{r}
log.mod = glm(diagnosis~., data = bcdf_fin, family = "binomial")
summary(log.mod)
```

Investigate multicollinearity problem

```{r}
bcdf_fin %>% dplyr::select(-diagnosis) %>%  #filter only numeric variables
  cor() %>%   
  #compute correlation matrix
  corrplot(method = "circle",         #represent correlation in "circle", size = magnitude 
           type = "upper", 
           diag=FALSE
           )
#We can see that there are some very strong correlations between certain variables
```


Find correlation pairs that are above 0.6 to leave out of the dataset

```{r}
#obtain list of variables that are correlated with one another whose correlation is at least 0.85
a = bcdf_x %>% 
    correlate() %>% 
    stretch() %>% 
    arrange(desc(r)) %>% 
    filter(r > 0.85) %>% 
    slice(which(row_number() %% 2 == 0)) %>% 
    pivot_longer(x:y) %>% dplyr::select(-r,-name) %>% distinct(value) 

#bcdf_x = as.data.frame(bcdf_x)[a$value]

new_bcdf = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -concave.points_worst, -concavity_worst, -compactness_worst, -compactness_mean, -diagnosis) 

#new_bcdf = as_tibble(bcdf_fin) %>% dplyr::select(compactness_mean, compactness_se, concave.points_se, concave.points_worst, concavity_se, fractal_dimension_mean, fractal_dimension_se, fractal_dimension_worst, radius_mean, radius_se, smoothness_mean, smoothness_se, smoothness_worst, symmetry_mean, symmetry_se, symmetry_worst, texture_mean, texture_se)

bcdf_with_y = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -concave.points_worst, -concavity_worst, -compactness_worst, -compactness_mean) 

bcdf_x = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -concave.points_worst, -concavity_worst, -compactness_worst, -compactness_mean, -diagnosis) %>% mutate(intercept = 1)

#bcdf_x = as_tibble(bcdf_fin) %>% dplyr::select(compactness_mean, compactness_se, concave.points_se, concave.points_worst, concavity_se, fractal_dimension_mean, fractal_dimension_se, fractal_dimension_worst, radius_mean, radius_se, smoothness_mean, smoothness_se, smoothness_worst, symmetry_mean, symmetry_se, symmetry_worst, texture_mean, texture_se) %>% mutate(intercept = 1)
```


### Task 1
\item Build a logistic model to classify the images into  malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.  

```{r}
# Function to compute the loglikelihood, the gradient, and the Hessian matrix for data dat evaluated at the parameter value betavec
## dat    - A list with components
#  x      - vector of explanatory variables
#  y      - vector of corresponding (binary) response variables
# betavec - [beta_0, beta_1, ..., beta_n] - the vector of parameter
#             values at which to evaluate these quantities

## Returns a list with the following components evaluated at beta
#  loglik - (scalar) the log likelihood
#  grad   - (vector of length 2) gradient
#  Hess   - (2 x 2 matrix) Hessian#
```

Function to return log-likelihood, gradient, and Hessian matrix of logistic regression

```{r}
logisticstuff <- function(y, x, betavec) {
  u <- x %*% betavec
  expu <- exp(u)
  loglik.ind = NULL
  #for(i in 1:length(y)) {
    #loglik.ind[i] = y[i]*u[i] - log(1+ expu)
    #}
  #loglik = sum(loglik.ind)
  loglik = t(u) %*% y - sum((log(1+expu)))
  # Log-likelihood at betavec
  
  p <- expu / (1 + expu)
  # P(Y_i=1|x_i)
  
  #grad = NULL
  #for(i in 1:length(betavec)){
    #grad[i] = sum(t(x[,i])%*%(dat$y - p))
  #}
  grad = t(x) %*% (y-p)
   #gradient at betavec
  
    # Hessian at betavec
  hess <- -t(x) %*% diag(as.vector(p*(1-p))) %*% x
  return(list(loglik = loglik, grad = grad, Hess = hess))
}
```

Newton-Raphson with gradient ascent and step-halving

```{r}
NewtonRaphson <- function(y, x, func, start, tol=1e-10, maxiter = 200) {
  i <- 0
  cur <- start
  x = as.matrix(x)
  colnames(x) = names(bcdf_x)
  stuff <- func(y, x , cur)
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf
  while(i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i <- i + 1
    prevloglik <- stuff$loglik
    prev <- cur
    grad <- stuff$grad
    hess <- stuff$Hess
    
    #gradient descent 
    if(t(grad) %*% hess %*% grad > 0){#positive definite matrix
    inv.hess = 
      solve(hess - (max(diag(hess))+100)*diag(nrow(hess)))} #make positive definite matrix negative definite
    else 
    {inv.hess <- solve(hess)}
    
    cur <- prev - inv.hess%*%grad
    stuff <- func(y, x, cur)
    
    #step-halving
    step = 0
    while (prevloglik > stuff$loglik){#moving too far -> halve step
    step = step + 1 
    cur <- prev - (1/2)^step * inv.hess%*%grad
    stuff <- func(y, x, cur)
    }
  res <- rbind(res, c(i, stuff$loglik, cur))
  }
  return(res)
  }
```

Test on dataset 
```{r}
newton_raph_res = NewtonRaphson(y = bcdf_fin$diagnosis, as.matrix(bcdf_x), logisticstuff, start = rep(0, ncol(bcdf_x)))

colnames(newton_raph_res) = names()
#start = rep(1, ncol(bcdf_x))
#as.matrix(bcdf_x) %*%rep(1, ncol(bcdf_x))

#t(as.matrix(bcdf_x)) %*% as.matrix(bcdf_fin[,1])
```


Logistic-LASSO

Coordinate-wise descent LASSO

```{r}
soft_threshold = function(beta, lambda) {
  ifelse(abs(beta)>lambda && beta > 0,
         beta-lambda, 
         ifelse(abs(beta) > lambda && beta < 0, 
                beta + lambda,
                0))}

#soft_threshold = function(beta, lambda) {sign(beta)*ifelse(abs(beta)>lambda, abs(beta)-lambda,0)}

#soft_threshold = function(beta, lambda) {sign(beta)*max(abs(beta)-lambda, 0)}

cord.lasso = function(lambda, y, X, betavec, tol = 1e-7, maxiter = 200){
  i = 0
  X = as.matrix(X)
  loglik = 1e6 
  res = c(0, loglik, betavec)
  prevloglik = Inf
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf){
    i = i + 1
    prevloglik = loglik
    for (k in 1:length(betavec)){
      u = X %*% betavec 
      expu = exp(u)
      p = expu/(1 + expu)
      weight = p*(1-p)
      
      #avoid coefficients from divergence to achieve final fitted probabilities of 0 or 1
      weight = ifelse(abs(weight-0) < 1e-7, 1e-7, weight)
      
      #calculate working responses z
      z = u + (y-p)/weight
      #r = z - X%*%betavec
      z_without_j = X[,-k] %*% betavec[-k]
      
      #soft-threshold solution
      betavec[k] = soft_threshold(mean(weight*X[,k]*(z-z_without_j)),lambda)/(mean(weight*(X[,k]^2)))}
    
    #calculate new log-likelihood
  loglik = 1/(2*nrow(X))*sum(weight*(z-X%*%betavec)^2) + lambda*sum(abs(betavec))
  res = rbind(res, c(i, loglik, betavec))}
  return(res)
}
```

```{r}
cord.lasso(lambda = 0.3, y = bcdf_fin$diagnosis, X = as.matrix(bcdf_x), betavec = rep(1, ncol(bcdf_x)))
```

### Check for convergence

Compute the solution on a grid of lambdas. Pathwise coordinate optimization to get PATH OF SOLUTIONS

```{r}
# compute the solution on a grid of lambdas
 #nl      <- 200
 #lambda  <- seq(0,200,length=nl)
 #beta_cd <- matrix(0,nl,p)

path = function(X, y, tunegrid){
  beta = NULL
  tunegrid = as.vector(tunegrid)
  for (nl in tunegrid){
    cor.result = cord.lasso(lambda = nl,
                            X = as.matrix(X), 
                            y = y,
                            betavec = rep(1, ncol(X)))
    last_beta = cor.result[nrow(cor.result),3:ncol(cor.result)]
    betavec = last_beta
    beta = rbind(beta, c(last_beta))
  }
  return(cbind(tunegrid,beta))
}

path.out = path(X = bcdf_x, y = bcdf_fin$diagnosis, tunegrid = exp(seq(0, -8, length = 100)))
colnames(path.out) = c("tunegrid", colnames(new_bcdf), "intercept")
path.out = as.data.frame(path.out)
```

Plot the path
```{r}
path.out %>% 
  pivot_longer(2:ncol(path.out), 
               names_to = "predictors",
               values_to = "estimate") %>% 
  ggplot(aes(x = log(tunegrid), y = estimate, group = predictors, col = predictors)) + 
  geom_line() + 
  labs(x = "log(lambda)",
       y = "Coefficient estimate")
```

#Cross validation

```{r}
mses = NULL
mse = NULL
rmse.std.error = NULL
grid = NULL
i = 0
crossval = function(X, y, tunegrid, fold_num){
  folds = sample(1:fold_num, nrow(X), replace = TRUE)
for(nl in tunegrid){
  i = i + 1
  for(k in 1:fold_num){
  start = rep(1, ncol(X))
  x_train = as.matrix(X[folds != k,])
  y_train = y[folds != k] 
  x_test = as.matrix(X[folds == k,]) 
  y_test = y[folds == k]
  loglasso_res = cord.lasso(lambda = nl, 
                            y = y_train, 
                            X = x_train, 
                            betavec = start)
  loglasso_coeff = loglasso_res[nrow(loglasso_res),3:ncol(loglasso_res)]
  expu = exp(x_test %*% loglasso_coeff)
  p = expu/(1+expu)
  mses[i] = mean((y_test-p)^2) #cross-validated MSE
  start = loglasso_coeff
  }
  mse[i] = mean(mses)
  rmse.std.error[i] = sqrt(var(mses)/fold_num)
  grid[i] = nl
  res = cbind(grid, mse, rmse.std.error)
}
  return(res)}

cv_res = crossval(X = bcdf_x, y = bcdf_fin$diagnosis, tunegrid = exp(seq(-6,1,length = 100)), fold_num = 5) %>% as_tibble()
```

Find best lambda 
```{r}
best.lambda = cv_res %>% filter(mse == min(cv_res$mse)) %>% dplyr::select(grid)

```

Visualize CV RMSE 
```{r}
cv_res %>% ggplot(aes(x = log(cv_res$grid), y = cv_res$mse)) + 
  geom_errorbar(aes(ymin = cv_res$mse-cv_res$rmse.std.error,
                    ymax = cv_res$mse+cv_res$rmse.std.error),col = 1) + 
  geom_line() + geom_point(size = 0.8, col = 4)
```


# Perform cross-validation logistic LASSO in glmnet (for comparison)

```{r}
cv.lasso <- cv.glmnet(as.matrix(bcdf_x), y = as.factor(bcdf_fin$diagnosis),
                      family="binomial",
                      type.measure = "mse",
                      alpha = 1, 
                      lambda = exp(seq(-10, -1, length=100)))

plot(cv.lasso)

cv.lasso$lambda.min
```

