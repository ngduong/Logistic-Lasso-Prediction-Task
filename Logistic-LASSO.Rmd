---
title: "Logistic-LASSO Breast Cancer Classification Task"
author: "Ngoc Duong - nqd2000"
date: "3/25/2020"
output: 
  pdf_document:
    keep_tex: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)
library(corrplot)
library(corrr)
```

Data import and cleaning 
```{r warning = FALSE, message = FALSE}
breast_cancer_data = read.csv("./breast-cancer-1.csv")

bcdf = breast_cancer_data %>% 
  mutate(diagnosis = ifelse(diagnosis == "M",1,0)) %>% 
  dplyr::select(diagnosis, everything()) %>% 
  dplyr::select(-id, -X)
```

Standardize design matrix (because although logistic is scale-invariant, LASSO is not, this is to ensure comparability of estimates by these different models)

```{r warning = FALSE, message = FALSE}
pred_names = bcdf %>% dplyr::select(-diagnosis) %>% names() %>% as.vector()
bcdf_x = NULL

for (i in pred_names) {
col = (bcdf[,i] - mean(bcdf[,i]))/sd(bcdf[,i])
bcdf_x = cbind(bcdf_x , col)
}

colnames(bcdf_x) <- c(pred_names)

bcdf_fin = cbind(bcdf[1], bcdf_x)
```

Investigate multicollinearity problem

```{r warning = FALSE, message = FALSE}
bcdf_fin %>% dplyr::select(-diagnosis) %>%  #filter only numeric variables
  cor() %>%   
  #compute correlation matrix
  corrplot(method = "circle",         #represent correlation in "circle", size = magnitude 
           type = "upper", 
           diag=FALSE
           )
#We can see that there are some very strong correlations between certain variables
```

Find correlation pairs that are above 0.85 to leave out of the dataset

```{r warning = FALSE, message = FALSE}
#obtain list of variables that are correlated with one another whose correlation is at least 0.85
cor_var = bcdf_x %>% 
    correlate() %>% 
    stretch() %>% 
    arrange(desc(r)) %>% 
    filter(r > 0.85) %>% 
    slice(which(row_number() %% 2 == 0)) %>% 
    pivot_longer(x:y) %>% dplyr::select(-r,-name) %>% distinct(value) 

#full data with response variable and predictors
full_data = as_tibble(bcdf_fin) %>% dplyr::select(-perimeter_mean, -radius_mean, -perimeter_worst, -radius_worst, -area_mean, -area_worst, -perimeter_se, -radius_se, -area_se, -concave.points_mean, -concavity_mean, -texture_worst, -texture_mean, -compactness_worst, -concave.points_worst, -concavity_worst,-compactness_mean) 

#design matrix without intercept
Xmat_no_int = full_data %>% dplyr::select(-diagnosis) 

#design matrix with intercept
Xmat_int = Xmat_no_int %>% mutate(intercept = 1)
```

Looking at logistic regression results by glm 
```{r warning = FALSE, message = FALSE}
log.mod = glm(diagnosis~., data = full_data, family = "binomial")
summary(log.mod)

glm_coeff_tib = tibble(`GLM binomial` = round(replace(log.mod$coeff %>% as.numeric(), c(1,2:14), log.mod$coeff[c(2:14,1)]),4))
```

### Task 1
Function to return log-likelihood, gradient, and Hessian matrix of logistic regression

```{r warning = FALSE, message = FALSE}
logisticstuff <- function(y, x, betavec) {
  u <- x %*% betavec
  expu <- exp(u)
  loglik.ind = NULL

  loglik = t(u) %*% y - sum((log(1+expu)))
  # Log-likelihood at betavec
  
  p <- expu / (1 + expu)
  # P(Y_i=1|x_i)
  grad = t(x) %*% (y-p)
   #gradient at betavec
  
    # Hessian at betavec
  hess <- -t(x) %*% diag(as.vector(p*(1-p))) %*% x
  return(list(loglik = loglik, grad = grad, Hess = hess))
}
```

Newton-Raphson with gradient descent and step-halving

```{r warning = FALSE, message = FALSE}
NewtonRaphson <- function(y, x, func, start, tol=1e-10, maxiter = 200) {
  i <- 0
  cur <- start
  x = as.matrix(x)
  colnames(x) = names(bcdf_x)
  stuff <- func(y, x , cur)
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf
  while(i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i <- i + 1
    prevloglik <- stuff$loglik
    prev <- cur
    grad <- stuff$grad
    hess <- stuff$Hess
    
    #gradient descent 
    if(t(grad) %*% hess %*% grad > 0){#positive definite matrix
    inv.hess = 
      solve(hess - (max(diag(hess))+100)*diag(nrow(hess)))} #make positive definite matrix negative definite
    else 
    {inv.hess <- solve(hess)}
    
    cur <- prev - inv.hess%*%grad
    stuff <- func(y, x, cur)
    
    #step-halving
    step = 0
    while (prevloglik > stuff$loglik){#moving too far -> halve step
    step = step + 1 
    cur <- prev - (1/2)^step * inv.hess%*%grad
    stuff <- func(y, x, cur)
    }
  res <- rbind(res, c(i, stuff$loglik, cur))
  }
  return(res)
  }
```

Test on dataset 
```{r warning = FALSE, message = FALSE}
newton_raph_res = NewtonRaphson(y = full_data$diagnosis, as.matrix(Xmat_int), logisticstuff, start = rep(0, ncol(Xmat_int)))

#convert to data frame 
newton_raph_coeff = newton_raph_res[c(nrow(newton_raph_res)),3:ncol(newton_raph_res)] %>% t() %>% as.data.frame()

#assign names to coeffcieints
colnames(newton_raph_coeff) = colnames(Xmat_int)

#obtain final coeffcients
nr_coeff_tib = as_tibble(round(newton_raph_coeff,4))
```


Logistic-LASSO

Coordinate-wise descent algorithm

```{r warning = FALSE, message = FALSE}
soft_threshold = function(beta, lambda) {
  ifelse(abs(beta)>lambda && beta > 0,
         beta-lambda, 
         ifelse(abs(beta) > lambda && beta < 0, 
                beta + lambda,
                0))}

#soft_threshold = function(beta, lambda) {sign(beta)*ifelse(abs(beta)>lambda, abs(beta)-lambda,0)}

coord.lasso = function(lambda, y, X, betavec, tol = 1e-7, maxiter = 200){
  i = 0
  X = as.matrix(X)
  loglik = 1e6 
  res = c(0, loglik, betavec)
  prevloglik = Inf
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf){
    i = i + 1
    prevloglik = loglik
    for (k in 1:length(betavec)){
      u = X %*% betavec 
      expu = exp(u)
      p = expu/(1 + expu)
      weight = p*(1-p)
      
      #avoid coefficients from divergence to achieve final fitted probabilities of 0 or 1
      weight = ifelse(abs(weight-0) < 1e-6, 1e-6, weight)
      
      #calculate working responses 
      resp = u + (y-p)/weight
      #r = z - X%*%betavec
      resp_without_j = X[,-k] %*% betavec[-k]
      
      #soft-threshold solution
      betavec[k] = soft_threshold(mean(weight*X[,k]*(resp-resp_without_j)),lambda)/(mean(weight*(X[,k]^2)))}
    
    #calculate new log-likelihood
  loglik = 1/(2*nrow(X))*sum(weight*(resp-X%*%betavec)^2) + lambda*sum(abs(betavec))
  res = rbind(res, c(i, loglik, betavec))
  }
  return(res)
}
```

```{r warning = FALSE, message = FALSE}
coord.lasso(lambda = 0.3, 
            y = full_data$diagnosis, 
            X = as.matrix(Xmat_int), 
            betavec = rep(1, ncol(Xmat_int)))
```

### Check for convergence

Compute the solution on a grid of lambdas. Pathwise coordinate optimization to get path of solutions

```{r warning = FALSE, message = FALSE}
path = function(X, y, tunegrid){
  coeff = NULL
  tunegrid = as.vector(tunegrid)
  for (nl in tunegrid){
    coord_res = coord.lasso(lambda = nl,
                            X = as.matrix(X), 
                            y = y,
                            betavec = rep(1, ncol(X)))
    last_beta = coord_res[nrow(coord_res),3:ncol(coord_res)]
    betavec = last_beta
    coeff = rbind(coeff, c(last_beta))
  }
  return(cbind(tunegrid, coeff))
}

path_df = path(X = Xmat_int, y = bcdf_fin$diagnosis, tunegrid = exp(seq(0, -8, length = 100)))
colnames(path_df) = c("Tunegrid", colnames(Xmat_no_int), "Intercept")
path_df = as.data.frame(path_df)
```

Plot the path
```{r warning = FALSE, message = FALSE}
path_df %>% 
  pivot_longer(2:ncol(path_df), 
               names_to = "Predictors",
               values_to = "estimate") %>% 
  ggplot(aes(x = log(Tunegrid), y = estimate, group = Predictors, col = Predictors)) + 
  geom_line() + 
  labs(x = "Log(lambda)",
       y = "Coefficient Estimate") 
```

#Cross validation

```{r warning = FALSE, message = FALSE}
set.seed(2020)
mses = NULL
mse = NULL
rmse.std.error = NULL
grid = NULL
i = 0
crossval = function(X, y, tunegrid, fold_num){
  folds = sample(1:fold_num, nrow(X), replace = TRUE)
for(nl in tunegrid){
  i = i + 1
  for(k in 1:fold_num){
  #start = rep(1, ncol(X))
  x_train = as.matrix(X[folds != k,])
  y_train = y[folds != k] 
  x_test = as.matrix(X[folds == k,]) 
  y_test = y[folds == k]
  start = rep(1, ncol(x_train))
  loglasso_res = coord.lasso(lambda = nl, 
                            y = y_train, 
                            X = x_train, 
                            betavec = start)
  loglasso_coeff = loglasso_res[nrow(loglasso_res),3:ncol(loglasso_res)]
  expu = exp(x_test %*% loglasso_coeff)
  p = expu/(1+expu)
  mses[k] = mean((y_test-p)^2) #cross-validated MSE
  start = loglasso_coeff
  }
  mse[i] = mean(mses)
  rmse.std.error[i] = sqrt(var(mses)/fold_num)
  grid[i] = nl
  res = cbind(grid, mse, rmse.std.error)
}
  return(res)}

cv_res = crossval(X = Xmat_int, y = full_data$diagnosis, tunegrid = exp(seq(-9,-2,length = 100)), fold_num = 5) %>% as_tibble()
```

Find best lambda 
```{r warning = FALSE, message = FALSE}
best.ll.lambda = cv_res %>% filter(mse == min(cv_res$mse)) %>% dplyr::select(grid)
best.ll.lambda
log(best.ll.lambda)
```

### Visualize CV RMSE 
```{r warning = FALSE, message = FALSE}
cv_res %>% ggplot(aes(x = log(cv_res$grid), y = cv_res$mse)) + 
  geom_errorbar(aes(ymin = cv_res$mse-cv_res$rmse.std.error,
                    ymax = cv_res$mse+cv_res$rmse.std.error),col = 1) + 
  geom_line() + geom_point(size = 0.8, col = 4) + 
  labs(x = "Log(lambda)", y = "Mean-Squared Error") + 
  geom_vline(xintercept = as.numeric(log(best.ll.lambda)), col = 2) + 
  geom_text(aes(x=as.numeric(log(best.ll.lambda)), label="log(lambda) = -5.4", y=0.22), col = 2, vjust = 2, text=element_text(size=11))
```


### Perform cross-validation logistic LASSO in glmnet (for comparison)

```{r warning = FALSE, message = FALSE}
set.seed(2020)
cv.lasso <- cv.glmnet(as.matrix(Xmat_no_int), y = as.factor(full_data$diagnosis),
                      family="binomial",
                      type.measure = "mse",
                      nfolds = 5,
                      alpha = 1, 
                      lambda = exp(seq(-9, -2, length=100)))

plot(cv.lasso)

cv.lasso$lambda.min
log(cv.lasso$lambda.min)

#coefficients
coeff = coef(cv.lasso, s=cv.lasso$lambda.min) %>% as.numeric()
glmnet_coeff = replace(coeff, c(1,2:14), coeff[c(2:14,1)])

#make tibble glmnet lasso coeff
glmnet_coeff_tib = tibble(`GLMnet` = round(glmnet_coeff,4))
```


### Calculate MSE for Logistic-Lasso and Newton Raphson

```{r warning = FALSE, message = FALSE}
pred_error = function(y, X, betavec) {
  expu = exp(as.matrix(X) %*% betavec)
  p = expu/(1+expu)
  prediction_error = mean((as.vector(y)-p)^2)
  return(prediction_error)
}
```

### Newton Raphon's MSE 

```{r warning = FALSE, message = FALSE}
newton_raph_vec = newton_raph_res[c(nrow(newton_raph_res)),3:ncol(newton_raph_res)]

#nr_coeff_tib = tibble(`Newton-Raphson` = round(newton_raph_res[c(nrow(newton_raph_res)),3:ncol(newton_raph_res)]),4) 
newton_raph_error = pred_error(full_data$diagnosis, Xmat_int, newton_raph_vec)
newton_raph_error
```

### Logistic-Lasso's MSE

```{r warning = FALSE, message = FALSE}
loglasso_betas = coord.lasso(lambda = as.numeric(best.ll.lambda), 
            y = full_data$diagnosis, 
            X = as.matrix(Xmat_int), 
            betavec = rep(1, ncol(Xmat_int)))
#get coefficients at best lambda 
loglasso_betas = loglasso_betas[nrow(loglasso_betas), 3:ncol(loglasso_betas)]

#make tibble 
loglasso_coeff_tib = tibble(`Logistic-LASSO` = round(loglasso_betas, 4))

#calc error
loglasso_error = pred_error(full_data$diagnosis, Xmat_int, loglasso_betas)
loglasso_error
```

GLMNet's MSE
```{r warning = FALSE, message = FALSE}
glmnet_error = pred_error(full_data$diagnosis, Xmat_int, glmnet_coeff)
glmnet_error
```

### Summary table
```{r warning = FALSE, message = FALSE}
log_lasso = c(error = round(loglasso_error,4))
newton_raphson = c(error = round(newton_raph_error,4))
glmnet = c(error = round(glmnet_error,4))

table2 = cbind(newton_raphson, log_lasso, glmnet)

rownames(table2) <- c("MSE")
colnames(table2)[1:3] <- c("Newton-Raphson","Logistic LASSO","GLMnet") 
knitr::kable(table2, escape = FALSE)
```


### Cross-validation

```{r warning = FALSE, message = FALSE}
nr_mses = NULL
ll_mses = NULL
glmnet_mses = NULL
error_comp_df = NULL
set.seed(2020)

#k-fold cross-validation
cv_comp = function(X, y, fold_num){
  folds = sample(1:fold_num, nrow(X), replace = TRUE)
  for (k in 1:fold_num){
  #start = rep(1, ncol(X))
  x_train = as.matrix(X[folds != k,])
  y_train = y[folds != k] 
  x_test = as.matrix(X[folds == k,]) 
  y_test = y[folds == k]
  
  ll_expu = exp(x_test %*% loglasso_betas)
  ll_p = ll_expu/(1+ll_expu)
  ll_mse = mean((y_test-ll_p)^2) #cross-validated MSE for logistic lasso
  ll_mses = rbind(ll_mses, ll_mse)
  
  nr_expu = exp(x_test %*% newton_raph_vec)
  nr_p = nr_expu/(1+nr_expu)
  nr_mse = mean((y_test - nr_p)^2) #cross-validated MSE for newton-raphson
  nr_mses = rbind(nr_mses, nr_mse)
  
  glmnet_expu = exp(x_test %*% glmnet_coeff)
  glmnet_p = glmnet_expu/(1+glmnet_expu)
  glmnet_mse = mean((y_test - glmnet_p)^2) #cross-validated MSE for glmnet
  glmnet_mses = rbind(glmnet_mses, glmnet_mse)
  }
  res = tibble(`GLMnet`= glmnet_mse, `Logistic-LASSO` = ll_mses, `Newton-Raphson` = nr_mses)
  return(res)}

#repeated cross-validation n times
rep_cv = function(X, y, fold_num, n){
  while (i <= n){
    i = i+1
    error_comp = cv_comp(X, y, fold_num)
    error_comp_df = rbind(error_comp_df, error_comp)
  }
  return(error_comp_df)
}

mse_comp_df = rep_cv(X = Xmat_int, y = full_data$diagnosis, fold_num = 5, n =1)

rep_mse_comp_df = rep_cv(X = Xmat_int, y = full_data$diagnosis, fold_num = 5, n =5)
```


### Visualize error comparisons
```{r warning = FALSE, message = FALSE}
mse_comp_df %>% 
  pivot_longer(1:3, names_to = "Model", values_to = "MSE") %>% 
  ggplot(aes(x = MSE, col = Model, fill = Model)) + 
  geom_density(adjust = 1.5, alpha = 0.3) + 
  labs(title = "Distribution of 5-fold cross-validated MSE across models",
       x = "Cross-validated MSE",
       y = "Density") +
  theme(legend.position = "bottom",
        plot.title = element_text(size= 11, hjust = 0.5))


rep_mse_comp_df %>% 
  pivot_longer(1:3, names_to = "Model", values_to = "MSE") %>% 
  ggplot(aes(x = MSE, col = Model, fill = Model)) + 
  geom_density(adjust = 1.5, alpha = 0.3) + 
  labs(title = "Distribution of repeated 5-fold cross-validated MSE across models",
       x = "Cross-validated MSE",
       y = "Density") +
  theme(legend.position = "bottom",
        plot.title = element_text(size= 11, hjust = 0.5))
```


### All coefficients by all model 

```{r warning = FALSE, message = FALSE}
table1 = cbind(glm_coeff_tib, t(nr_coeff_tib), glmnet_coeff_tib, loglasso_coeff_tib) %>% rename_at(2, ~"Newton-Raphson")

knitr::kable(table1, escape = FALSE)
```


### ROC curves 

```{r warning = FALSE, message = FALSE}
nr_pred_prob <- predict(log.mod, newdata = full_data, type = "response")

roc.glm <- roc(full_data$diagnosis, nr_pred_prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
```


